# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZsoDcp3lai7dcLp6MViY460hX_6_6cMS
"""

import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Define the transformations (convert to tensor and normalize)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std
])

# Load training data
train_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

# Load test data
test_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform
)

# Create data loaders
train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=64,
    shuffle=False
)

Model = nn.Sequential(
    nn.Linear(28**2, 128, device),
    nn.ReLU(),
    nn.Linear(128, 128, device),
    nn.ReLU(),
    nn.Linear(128, 10, device)
).to(device)

i = 10

plt.imshow(train_loader.dataset.data[i])

x = train_loader.dataset.data[i].view(-1, 28 ** 2).type(torch.float32)

y = Model(x)
print(torch.argmax(y))

def train(model, epochs=1):
  optimizer = optim.SGD(model.parameters(), lr=0.01)
  loss_fn = nn.CrossEntropyLoss()
  train_losses = []
  test_losses = []

  for epoch in range(epochs):
    model.train()
    for i, (data, target) in enumerate(train_loader):
      targets = F.one_hot(target, num_classes=10).type(torch.float32)

      optimizer.zero_grad()
      y_preds = model(data.view(-1, 28 ** 2))
      loss = loss_fn(y_preds, targets)
      loss.backward()
      optimizer.step()

      train_losses.append(loss.item())

    model.eval()
    with torch.no_grad():
      for i, (data, target) in enumerate(test_loader):
        targets = F.one_hot(target, num_classes=10).type(torch.float32)

        y_preds = model(data.view(-1, 28 ** 2))
        loss = loss_fn(y_preds, targets)

        test_losses.append(loss.item())

  return (train_losses, test_losses)

(train_losses, test_losses) = train(Model)

xs = torch.arange(20)

plt.figure(figsize=(10, 6))  # Creates a figure with specified size
plt.plot(train_losses)  # Plot the actual curve
plt.title('Training Loss Over Time')  # Add a title
plt.xlabel('Epoch')  # Label for x-axis
plt.ylabel('Loss')  # Label for y-axis
plt.grid(True)  # Add a grid for better readability
plt.show()  # Display the plot

plt.figure(figsize=(10, 6))  # Creates a figure with specified size
plt.plot(test_losses)  # Plot the actual curve
plt.title('Test Loss Over Time')  # Add a title
plt.xlabel('Epoch')  # Label for x-axis
plt.ylabel('Loss')  # Label for y-axis
plt.grid(True)  # Add a grid for better readability
plt.show()  # Display the plot

i = 20029

plt.imshow(train_loader.dataset.data[i])

x = train_loader.dataset.data[i].view(-1, 28 ** 2).type(torch.float32)

y = Model(x)
print(torch.argmax(y))
